{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Venâncio Freitas de Araújo Filho\n",
    "\n",
    "Nome: Wellington Rodrigues da Silva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importando as Bibliotecas e fazendo um reconhecimentos das bases de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importante e baixando Bibliotecas que serão testadas ou utilizadas no projeto: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\venan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\venan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\venan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import unicodedata\n",
    "import nltk\n",
    "import re \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diretório\n",
      "C:\\Users\\venan\\OneDrive - Insper - Institudo de Ensino e Pesquisa\\Materias 2° semestre Insper\\Ciência de dados\\Projetos\\Classificador-Naive-Bayes-Amazon\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diretório')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tratamento dos dados para treinamentos do boot: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando a base de dados de  com os tweets classificados manualmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dados_treino.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1056\\3448893223.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dados_treino.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1374\u001b[0m                 \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xls\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1376\u001b[1;33m                 ext = inspect_excel_format(\n\u001b[0m\u001b[0;32m   1377\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1248\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m     with get_handle(\n\u001b[0m\u001b[0;32m   1251\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m     ) as handle:\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    796\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dados_treino.xlsx'"
     ]
    }
   ],
   "source": [
    "train = pd.read_excel('dados_treino.xlsx')\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisando a proporção dos valores nessa base de dados:\n",
    "\n",
    "\n",
    "Nessa etapa, é relevante preocupar-se com uma distribuição semelhante entre os rótulos utilizados, para evitar discrepâncias no posterior cálculo de probabilidade, que será explicado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Target.value_counts(True).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tratamento dos dados da base de dados do Teste do Bot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_excel('dados_teste.xlsx')\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisando a proporção dos valores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.Target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador Automático de Mensagens\n",
    "\n",
    "Diante do cenário de digitalização global, é relevante para as plataformas a preocupação com a experiência dos clientes. Assim, coube ao grupo em questão, otimizar o processo de identificação de possíveis feedbacks dos clientes.Assim,projetou-se um Bot para, a partir das palavras da mensagem de feedback digitada pelo cliente na plataforma de serviço Amazon, encaminhá-la para 3 diferentes grupos, que, a partir da análise da mensagem, discutirão melhorias na experiência do cliente. Os 3 grupos são: \n",
    "\n",
    "- Grupo 1: Autor do Livro em questão \n",
    "- Grupo 2: Editora do Livro em questão \n",
    "- Grupo 3: Serviços prestados pela plataforma Amazon\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Configurações Iniciais: \n",
    "\n",
    "Para iniciar o projeto, notou-se que há uma grande quantidade de caracteres nas bases de dados, que dificultam que o tratamento seja feita de uma maneira, de fato, adequada , visto que, não agregam em nada à mensagem que o cliente busca transmitir na crítica. \n",
    "\n",
    "Assim, foram elaboradas funções que realizassem essa 'limpeza' do texto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Funções de Limpeza do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função 1 : Retirada de pontuações das mensagens\n",
    "\n",
    "Esse é um passo interessante, pois, as pontuações não transmitem nenhuma informação de relevância para a plataforma de serviços em questão, visto que não transmitem críticas a questões qualitativas do serviço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    \"\"\"\n",
    "        Função de limpeza muito simples que troca alguns sinais básicos por espaços\n",
    "    \"\"\"\n",
    "    #import string\n",
    "    punctuation = '[´\"!-.:?;$'']' # Note que os sinais [] são delimitadores de um conjunto.\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, '', text)\n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função 2: Extração do Radical de cada palavra\n",
    "\n",
    "Muito relevante pois, palavras com mesma origem, que partem de sufixos diferentes, sem essa abordagem, receberiam classificações diferentes. Porém, com a função Steamming, esse tipo de erro 'simples' é evitado pelo classificador.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steamming(texto): #Recebe lista \n",
    "    stemmed_palavras = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for palavra in texto:\n",
    "        \n",
    "        stemmed_palavras.append(stemmer.stem(palavra)) \n",
    "    \n",
    "    return stemmed_palavras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função 3: Extração de 'Stopwords' do texto\n",
    "\n",
    "Função que busca extrair palavras 'sem significado' do texto.É relevante para contextos específicos, como por exemplo, análise crítica de livros, visto que foca nas palavras que, de fato, expressam valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords2(texto): #Recebe lista \n",
    "    palavras_sem_stopwords = []\n",
    "    palavras = word_tokenize(' '.join(texto), language='portuguese')\n",
    "\n",
    "    stopwords_pt = set(stopwords.words('portuguese'))\n",
    "    \n",
    "    for palavra in palavras:\n",
    "        if palavra not in stopwords_pt:\n",
    "            palavras_sem_stopwords.append(palavra)\n",
    "    \n",
    "    return palavras_sem_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função 4: Remoção de Emojis dos textos \n",
    "\n",
    "Retirada de emojis do texto, já que os emojis não expressam mensagens que, de fato, possam ser traduzidas, por a sua interpretação ser muito subjetiva e, por complementar termos em muitas ocasiões. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(texto): #Recebe lista \n",
    "    texto = ' '.join(texto)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # Símbolos e pictogramas diversos\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # Símbolos alquímicos\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Símbolos de palavras\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Símbolos de árabes estendidos-A\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # Símbolos suplementares de árabes estendidos-B\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Símbolos de xadrez\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Símbolos suplementares de xadrez\n",
    "                           u\"\\U0001F004-\\U0001F0CF\"  # Símbolos de domino\n",
    "                           u\"\\U0001F170-\\U0001F251\"  # Símbolos de tai-xi\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # Símbolos e pictogramas diversos\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # Transporte e mapas\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # Símbolos alquímicos\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Símbolos de palavras\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Símbolos de árabes estendidos-A\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Símbolos de xadrez\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Símbolos suplementares de xadrez\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    emoji_pattern = emoji_pattern.sub(r'', texto)\n",
    "    emoji_pattern = emoji_pattern.split(' ')\n",
    "    return emoji_pattern\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função 5: Remoção de Acentuação das Palavras\n",
    "\n",
    "Retirada de acentuação das palavras contidas no texto, ja que a maioria das palavras escritas são de maneira informação, sem acentuação e as que possuem acentuação acabam alterando o resultado obtido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_acentuacao(texto):\n",
    "    texto_sem_acentos = ''.join(c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn')\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', texto_sem_acentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Funções de extração dos textos do Dataframe: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função 1: Limpeza do Dataframe\n",
    "\n",
    "Responsável por retirar as mensagens dos Dataframes, e separá-las. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracao_texto(series): \n",
    "    texto_novo = ''\n",
    "    for linha in series:\n",
    "        texto_novo += linha + ' '\n",
    "    return texto_novo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função 2: Extração e limpeza dos textos para cada Rótulo\n",
    "\n",
    "Otimização dos números de linhas de código utilizada para cada rótulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforma_rotulo(dataframe , string_rotulo):\n",
    "    dados_rotulo = dataframe.loc[dataframe.Target == string_rotulo , : ]\n",
    "    texto_rotulo = extracao_texto(dados_rotulo.Mensagem)\n",
    "    texto_rotulo = cleanup(texto_rotulo)\n",
    "    texto_rotulo = texto_rotulo.lower()\n",
    "    texto_rotulo = remove_acentuacao(texto_rotulo)\n",
    "    texto_rotulo = texto_rotulo.split(' ')\n",
    "    texto_rotulo = remove_emojis(texto_rotulo)\n",
    "    #texto_rotulo = stopwords2(texto_rotulo)\n",
    "    texto_rotulo = steamming(texto_rotulo)\n",
    "    tabela_rotulo = pd.Series(texto_rotulo)\n",
    "    return [tabela_rotulo , texto_rotulo]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montando um classificador Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento dos dados de cada rótulo\n",
    "\n",
    " - Com as configurações das funções montadas, iniciou-se a etapa do tratamento das mensagens referentes à cada rótulo,classificados manualmente. Nessa etapa, será utilizada a base de dados de treinamento, com maior tamanho em relação à base de teste. Fará o robô identificar as frequências das palavras frequentes em cada rótulo e, a partir disso, esse valor será utilizado no cálculo de probabilidade posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tratamento dos rótulos do Autor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extraindo os dados e implementando as limpezas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Montando um Texto\n",
    "tabela_autor = transforma_rotulo(train , 'Autor')[0]\n",
    "texto_autor = transforma_rotulo(train , 'Autor')[1]\n",
    "print(tabela_autor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tabela de palavras com frequência absoluta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequências Absolutas \n",
    "tabela_autor_abs = tabela_autor.value_counts()\n",
    "tabela_autor_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tabela de palavras com Frequência relativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Frequências relativas \n",
    "tabela_autor_relativa = tabela_autor.value_counts(True)\n",
    "tabela_autor_relativa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Número de palavras no Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_autor_abs.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tratamentos dos dados do rótulo da Editora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extração dos dados da Editora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tabela_editora = transforma_rotulo(train , 'Editora')[0]\n",
    "texto_editora = transforma_rotulo(train , 'Editora')[1]\n",
    "print(tabela_editora.head())\n",
    "print(tabela_editora.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tabela de frequências absolutas das palavras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequência absoluta\n",
    "tabela_editora_abs = tabela_editora.value_counts()\n",
    "tabela_editora_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tabela de Frequências relativas das palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequência Relativa \n",
    "tabela_editora_relativa = tabela_editora.value_counts(True)\n",
    "tabela_editora_relativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantidade de palavras \n",
    "tabela_editora_abs.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tratamento dos dados do Rótulo da Amazon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extraindo os dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tabela_amazon = transforma_rotulo(train , 'Amazon')[0]\n",
    "texto_amazon = transforma_rotulo(train , 'Amazon')[1]\n",
    "print(tabela_amazon.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Frequências absolutas das palavras:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequência absoluta \n",
    "tabela_amazon_abs = tabela_amazon.value_counts()\n",
    "tabela_amazon_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Frequência relativa das palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequência relativa\n",
    "tabela_amazon_relativa = tabela_amazon.value_counts(True)\n",
    "tabela_amazon_relativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Número de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_amazon_abs.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Construindo o Classificador a partir dos testes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lista_sem_repetição(lista_palavras_totais):\n",
    "    lista_sem_repeticao = []\n",
    "    for palavras in lista_palavras_totais: \n",
    "        if palavras not in lista_sem_repeticao:\n",
    "            lista_sem_repeticao.append(palavras)\n",
    "    return lista_sem_repeticao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todas_palavras = texto_editora + texto_amazon + texto_autor\n",
    "lista_sem_repeticao = lista_sem_repetição(todas_palavras)\n",
    "len(lista_sem_repeticao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Probabilidade de Cada Rótulo dentro do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_rotulo(lista_todas_palavras , tabela_rotulos):\n",
    "    probabilidade = sum(tabela_rotulos) / len(lista_todas_palavras)\n",
    "    return probabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Autor = prob_rotulo(todas_palavras , tabela_autor_abs)\n",
    "P_Autor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Amazon = prob_rotulo(todas_palavras , tabela_amazon_abs)\n",
    "P_Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_Editora = prob_rotulo(todas_palavras , tabela_editora_abs)\n",
    "P_Editora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Probabilidade de aparecer em cada frase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função da Suavização "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suavizacao(frase_linha , tabela_frequencia_abs , alpha , lista_sem_repetição , lista_palavras_rotulos):\n",
    "    P_frase_dado_rótulo = 1 \n",
    "    for palavra in frase_linha: \n",
    "        if palavra in lista_palavras_rotulos: \n",
    "            P_frase_dado_rótulo *= (tabela_frequencia_abs[palavra] + alpha)/ ( (len(lista_sem_repeticao) * alpha) + len(lista_palavras_rotulos))\n",
    "        else: \n",
    "            P_frase_dado_rótulo *= alpha/(len(lista_palavras_rotulos) +(len(lista_sem_repeticao)* alpha))\n",
    "    return P_frase_dado_rótulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vencedor (lista_probs):\n",
    "    if max(lista_probs) == lista_probs[0]:\n",
    "        return 'Autor'\n",
    "    elif max(lista_probs) == lista_probs[1]:\n",
    "        return 'Editora'\n",
    "    elif max(lista_probs) == lista_probs[2]:\n",
    "        return 'Amazon'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_bot(dataframe , alpha , lista_rotulos , lista_sem_repeticao , lista_textos):\n",
    "    \n",
    "    #Extraindo as tabelas rótulos\n",
    "    tabela_autor_abs = lista_rotulos[0]\n",
    "    tabela_editora_abs = lista_rotulos[1]\n",
    "    tabela_amazon_abs = lista_rotulos[2]\n",
    "    \n",
    "    #Extraindo os textos dos rótulos \n",
    "    \n",
    "    texto_autor = lista_textos[0]\n",
    "    texto_editora = lista_textos[1]\n",
    "    texto_amazon = lista_textos[2]\n",
    "    \n",
    "    \n",
    "    P_frase_dado_autor = 1\n",
    "    P_frase_dado_editora = 1\n",
    "    P_frase_dado_amazon = 1\n",
    "    lista_teste = []\n",
    "    lista_probs = []\n",
    "    for frase in dataframe.Mensagem:\n",
    "        #CleanUps\n",
    "        frase = cleanup(str(frase))\n",
    "        frase = frase.lower()\n",
    "        frase = remove_acentuacao(frase)\n",
    "        frase = frase.split(' ')\n",
    "        #frase = stopwords2(frase)\n",
    "        frase = remove_emojis(frase)\n",
    "        frase = steamming(frase)\n",
    "        \n",
    "        #Probabilidade de frase dado rótulo\n",
    "        P_frase_dado_autor = suavizacao(frase , tabela_autor_abs , alpha , lista_sem_repeticao , texto_autor)\n",
    "        P_frase_dado_editora = suavizacao(frase , tabela_editora_abs , alpha , lista_sem_repeticao , texto_editora)    \n",
    "        P_frase_dado_amazon = suavizacao(frase, tabela_amazon_abs , alpha , lista_sem_repeticao , texto_amazon) \n",
    "        \n",
    "        #Probabilidade de rótulo dado frase         \n",
    "        P_autor_dado_frase = P_frase_dado_autor * P_Autor\n",
    "        lista_probs.append( P_autor_dado_frase)\n",
    "    \n",
    "        P_editora_dado_frase = P_frase_dado_editora * P_Editora\n",
    "        lista_probs.append( P_editora_dado_frase)\n",
    "    \n",
    "        P_amazon_dado_frase =  P_frase_dado_amazon * P_Amazon\n",
    "        lista_probs.append( P_amazon_dado_frase)\n",
    "        \n",
    "        #Definindo o resultado do Robô\n",
    "        maior = vencedor(lista_probs)\n",
    "        lista_teste.append(maior)\n",
    "    \n",
    "        #Reiniciando as variáveis \n",
    "        P_frase_dado_editora = 1\n",
    "        P_frase_dado_amazon = 1\n",
    "        P_frase_dado_autor = 1\n",
    "    \n",
    "        P_autor_dado_frase = 1\n",
    "    \n",
    "        P_editora_dado_frase = 1\n",
    "    \n",
    "        P_amazon_dado_frase =  1\n",
    "        lista_probs = []\n",
    "    return lista_teste \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_teste = loop_bot(test , 1 , [tabela_autor_abs , tabela_editora_abs , tabela_amazon_abs] , lista_sem_repeticao , [texto_autor , texto_editora , texto_amazon])\n",
    "lista_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agora, Vamos Verificar a Performance do Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifica_acurácia(tabela_crosstab): \n",
    "    Acurácia = 0 \n",
    "    Acurácia += tabela_crosstab.Amazon[0]\n",
    "    Acurácia += tabela_crosstab.Autor[1]\n",
    "    Acurácia += tabela_crosstab.Editora[2]\n",
    "    return Acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test['Bot'] = lista_teste\n",
    "tabela_verificação = (pd.crosstab(test.Target , test.Bot , normalize = True )*100).round(3)\n",
    "print(tabela_verificação)\n",
    "print(verifica_acurácia(tabela_verificação))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificando os verdadeiros positivos para o caso Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_amazon_dado_amazon = tabela_verificação.Amazon.Amazon/sum(tabela_verificação.Amazon)\n",
    "P_amazon_dado_amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificando os casos de Falso Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_amazonC_dado_amazon = 1 - P_amazon_dado_amazon\n",
    "P_amazonC_dado_amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verficando verdadeiros positivos para o caso Autor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_autor_dado_autor = tabela_verificação.Autor.Autor/sum(tabela_verificação.Autor)\n",
    "P_autor_dado_autor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificando os casos de Falso Autor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_autorC_dado_autor = 1 - P_autor_dado_autor\n",
    "P_autorC_dado_autor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificação de verdadeiros positivos para o caso Editora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_editora_dado_editora = tabela_verificação.Editora.Editora/sum(tabela_verificação.Editora)\n",
    "P_editora_dado_editora "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificando os casos de Falso Autor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_editoraC_dado_editora = 1 - P_editora_dado_editora\n",
    "P_editoraC_dado_editora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluindo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diante da criação do classificador automático, cujo principal objetivo foi a determinação do destinatário para o qual o comentário do usuário se dirigia, tal que, de primeiro momento foi-se divido a classificação em Autor, Editora e Amazon, e com isso foi obtido uma acurácia de 69,195 %. Contudo, é valido ressaltar que ao empregar a função stopwords, um dos exemplos dados no guia do projeto, cujo seu resultado é dado na transformação de palavras em seu radical, obteve-se uma acurácia de 65.877%, com isso, conclui-se que como se trata de uma base de dados onde escritas informais são corriqueiras, algo que inibisse essa visualização para o aprendizado do algoritimo tornaria mais difícil e errônea a taxa de acerto de suas classificações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por que não podemos usar o próprio classificador para gerar mais amostras de treinamento?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visto que nesse determinado contexto, foi-se obtido uma acurácia de 69,195 %; ou seja, onde cerca de 69% das classificações feita pelo classificador eram iguais às classificações feitas a mão, temos que 31% das situações em que o classificador teve problemas para determinar o destinário, poderiam ser postergadas para novas amostras criadas por ele, dessa forma teríamos um classificador cada vez menos preciso. Ademais, visto que a base de dados não é grande o suficiente para um maior \"aprendizado\" do algoritmo torna-se ainda menos acertivo as rotulações dadas pelo classificador. Por fim, seria necessário a rotulação manual de cada base de dados, para que dessa forma os problemas de classificações sejam arrefecidos e com uma maior base de dados possa-se obter uma maior acurácia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferentes cenários de usabilidade para o sistema de classificação Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma das usabilidades do sistema Naive Bayes é sua aplicabilidade na área da Saúde, de forma que seja possível o processamento de um grande número de dados de tal maneira que informações uteis são fornecidas para tomada de decisões. De tal modo que por meio de uma base de dados relevante sobre uma determinada doença, e os indícios característicos daquela infermidade, torna o algoritimo capaz de identificar a probabilidade diante de padrões observados e situações correlacionadas do paciente estar com aquela determinada doença, e com o diagnóstico precoce, o tratamento preventivo reduz o agravamento da situação discrita. Ademais, no setor farmacêutico é válido destacar que a usabilidade do sistema Naive Bayes é crucial identificar caractrísticas específicas em determinada porcentagem de afetados por uma determinada doença e com isso otimizar a produção de medicamentos que visem atingir um maior número de pessoas.\n",
    "\n",
    "Outrossim, cabe destacar a usabilidade do algoritimo na previsão do tempo, visto que, diante da coleta de dados históricos e variáveis comuns para determinada situação a ser analisada ao longo de um período é possível a classificação por meio de classes, tais como: \"Dia ensolarado\", \"Fortes Chuvas\", entre outros.\n",
    "\n",
    "Por fim, cabe o destaque do classificador no reconhecimento de imagens, onde por meio de uma base de dados, ele identifica padrões encontrados em uma determinada categoria e pode apresentar ao usuário cenários parecidos, ou classificações específicas. Trazendo para o nosso cotidiano temos o uso do Google Lens, onde é possível fazer uma pesquisa utilizando uma imagem, ou seja, o algoritimo faz o uso dos padrões identificados para que seja viável o fornecimento de informações acerca daquela forma.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento de Mensagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Caso de Sarcasmo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante o desenvolvimento do sistema Naive Bayes e de acordo com o artigo Naive Bayes and Text Classification, nota-se que o algoritimo leva em consideração somente a frequência das palavras e as probabilidades condicionais as quais estão relacionadas, dessa maneira, mesmo que haja uma base de dados na qual há ocorência de palavras com sarcasmo, o classificador teria dificiculdade de obter êxito na previsão de determinada classe, visto que ele não leva em conta a intenção, ênfase e intonação, fatores que caracterizam um sarcasmo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Caso de Dupla Negação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levando em conta as características apresentadas pelo algoritimo, as quais foram destacadas acima, não é possível uma analise precisa por parte do classificador, visto que palavras negativas, levam a exclusão de determinado grupo, ou seja, um advérbio de negação seguido de uma classe pré determinada poderia levar o algaritimo a oferecer uma classe diferente da esperada e consequente um menor acurácia sobre a categorização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sugestões de melhoria do classificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Metódo de utilização do N-gramas de palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado um determinado contexto no qual se deseja analisar, temos a possiblidade de fazer o uso do método de N-gramas, em que o termo \"N\" é o número de elementos que se deseja analisar. Ademais, o método se baseia na divisão do texto observado em sequência de palavras sejam elas de forma consecutiva ou sequencial e dessa forma a identificação de palavras-chave que leve a uma determinada torna a classificação mais fácil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Para implementação deste método no código, precisamos fazer a utilização da biblioteca NLTK, a qual pode ser instalada da seguinte maneira:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"pip install nltk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Posteriormente, torna-se necessário a importação da biblioteca, que é feito da seguinte forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Por fim, por meio de uma função pode-se dividir o texto em palavras e gerar a quantidade de N-gramas desejados:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Palavras = nltk.word_tokenize(text1) #por meio da função tokenize, divido meu texto em palavras\n",
    "\n",
    "def gerar_bigramas(text2, n): # função que recebe como argumento o texto em forma de string e a quantidade desejada de N-gramas\n",
    "    n_grams = ngrams(text, n) # divisão por meio da função ngrams importado de nltk\n",
    "\n",
    "    return [' '.join(gram) for gram in n_grams]\n",
    "bigrams = gerar_bigramas(Palavras, 2) # dessa forma tenho como retorno bigramas das palavras passadas  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonte : \"Speech and Language Processing\" de Daniel Jurafsky e James H. Martin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Metódo TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse método é dividido em duas análises: TF, na qual é medido a frequência que a palavra aparece no nosso objeto de análise e IDF que calcula a raridade da aparição de determinado termo naquele todo. Com estas analises em mãos temos a importância relativa do termo no documento de analise e consequentemente a classificação feita pelo algoritimo é facilitada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Para implementação deste método no código, precisamos fazer a utilização da biblioteca NLTK, a qual pode ser instalada da seguinte maneira:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Posteriormente, torna-se necessário a importação da classe \"TfidfVectorizer\", que é feito da seguinte forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Por fim, por meio da implmentação da função abaixo :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def calcula_tf_idf(lista_frases): #recebe uma lista de frases\n",
    "\n",
    "    tf_idf = vectorizer.fit_transform(lista_frases) # faz o calculo da importancia relativa de cada termo\n",
    "    \n",
    "    return tf_idf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonte: Understanding TF-ID: A Simple Introduction - Monkey Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando o Robô para diferentes situações "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_final = pd.read_excel('planilha_junta.xlsx')\n",
    "lista_acurácias = []\n",
    "for i in range(100):\n",
    "    dataframe_dividido = train_test_split(dataframe_final , test_size =0.4 , shuffle = True)\n",
    "    train_final = dataframe_dividido[0]\n",
    "    test_final = dataframe_dividido[1]\n",
    "    #Extração das tabelas \n",
    "    tabela_autor = transforma_rotulo(train_final , 'Autor')[0]\n",
    "    texto_autor = transforma_rotulo(train_final , 'Autor')[1]\n",
    "    \n",
    "    tabela_editora = transforma_rotulo(train_final , 'Editora')[0]\n",
    "    texto_editora = transforma_rotulo(train_final , 'Editora')[1]\n",
    "\n",
    "    tabela_amazon = transforma_rotulo(train_final , 'Amazon')[0]\n",
    "    texto_amazon = transforma_rotulo(train_final , 'Amazon')[1]\n",
    "\n",
    "    #Extraindo as tabelas e as Palavras \n",
    "    #Tabelas Absolutas \n",
    "\n",
    "    tabela_autor_abs = tabela_autor.value_counts()\n",
    "    tabela_editora_abs = tabela_editora.value_counts()\n",
    "    tabela_amazon_abs = tabela_amazon.value_counts()\n",
    "    #Tabelas Relativas \n",
    "    tabela_autor_relativa = tabela_autor.value_counts(normalize=True)\n",
    "    tabela_editora_relativa = tabela_editora.value_counts(normalize=True)\n",
    "    tabela_amazon_relativa = tabela_amazon.value_counts(normalize=True)\n",
    "\n",
    "    #Todas as palavras\n",
    "    todas_palavras = texto_amazon + texto_autor + texto_editora\n",
    "    lista_sem_repeticao = lista_sem_repetição(todas_palavras)\n",
    "    \n",
    "    #Iniciando o cálculo de probabilidade \n",
    "    lista_resultados = loop_bot(test_final , 1 , [tabela_autor_abs , tabela_editora_abs , tabela_amazon_abs] , lista_sem_repeticao , [texto_autor , texto_editora , texto_amazon] ) \n",
    "    #1.Probabilidade de cada rótulo \n",
    "    P_Autor = prob_rotulo(todas_palavras, tabela_autor_abs)\n",
    "    P_Editora = prob_rotulo(todas_palavras , tabela_editora_abs)\n",
    "    P_Amazon = prob_rotulo(todas_palavras,  tabela_amazon_abs)\n",
    "    \n",
    "    #Montando a lista de resultados  \n",
    "    \n",
    "    #Montando crosstab \n",
    "    test_final['Bot'] = lista_resultados \n",
    "    tabela_resultados = (pd.crosstab(test_final.Target , test_final.Bot , normalize=True) * 100).round(3)\n",
    "    acurácia = verifica_acurácia(tabela_resultados)\n",
    "    lista_acurácias.append(acurácia)\n",
    "    acurácia = 0 \n",
    "    \n",
    "len(lista_acurácias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_boot = pd.DataFrame(lista_acurácias).round(3)\n",
    "resultados_boot_desempenho = resultados_boot.describe()\n",
    "resultados_boot_desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faixas = range(40 , 90 , 2)\n",
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.hist(resultados_boot , edgecolor='white' , color='Blue')\n",
    "plt.title('Distribuições dos valores do boot')\n",
    "plt.xlabel('Valores de Acurácia (%)')\n",
    "plt.ylabel('Frquência dos valores')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(resultados_boot , edgecolor='white' , density=True , color='Blue')\n",
    "plt.title('Distribuições dos valores do boot')\n",
    "plt.xlabel('Valores de Acurácia (%)')\n",
    "plt.ylabel('Frquência dos valores')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(resultados_boot , edgecolor='white' ,  bins=faixas , color='Blue')\n",
    "plt.title('Distribuições dos valores do boot')\n",
    "plt.xlabel('Valores de Acurácia (%)')\n",
    "plt.ylabel('Frequência dos valores')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aperfeiçoamento:\n",
    "\n",
    "Trabalhos que conseguirem pelo menos conceito B vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* IMPLEMENTOU outras limpezas e transformações que não afetem a qualidade da informação contida nos tweets. Ex: stemming, lemmatization, stopwords\n",
    "* CONSIDEROU mais de duas categorias na variável Target e INCREMENTOU a quantidade de notícias, mantendo pelo menos 250 notícias por categoria (OBRIGATÓRIO PARA TRIOS, sem contar como item avançado)\n",
    "* Para Target com duas categorias: CRIOU pelo menos quatro categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante\n",
    "* EXPLICOU porquê não pode usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* PROPÔS diferentes cenários para Naïve Bayes fora do contexto do projeto (pelo menos dois cenários, exceto aqueles já apresentados em sala pelos professores: por exemplo, filtro de spam)\n",
    "* SUGERIU e EXPLICOU melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* FEZ o item Qualidade do Classificador a partir de novas separações das Notícias entre Treinamento e Teste descrito no enunciado do projeto (OBRIGATÓRIO para conceitos A ou A+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
